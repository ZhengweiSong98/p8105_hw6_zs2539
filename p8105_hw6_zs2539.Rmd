---
title: "HW6 Solution"
author: "Zhengwei Song"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(rstatix)
library(PerformanceAnalytics)
library(modelr)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 

## Problem 2

## Importing the dataset
```{r, warning = FALSE, message=FALSE}
homicide_raw <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

#### Creating a `city_state` variable, binary variable `resolution` indicating case disposition, omitting cities excluded victim race and Tulsa, AL with data entry mistake. Also, limiting `victim_race` is white or black, making `victim_age` numeric.
```{r, warning = FALSE, message=FALSE}
homicide_df = homicide_raw %>% 
    janitor::clean_names() %>%
    mutate(
        reported_date = as.Date(as.character(reported_date), format = "%Y%m%d"),
        city_state = str_c(city, state, sep = ", "),
        resolved = as.numeric(disposition == "Closed by arrest"),
        victim_age = as.numeric(victim_age),
        victim_race = fct_relevel(victim_race, "White")
    ) %>%
    relocate(city_state) %>%
    filter(!city_state %in% c("Dallas, TX","Phoenix, AZ","Kansas City, MO","Tulsa, AL"),
         victim_race %in% c("White","Black"))
```

#### Applying `glm` to fit a logistic regression with resolved vs unresolved as the outcome; victim age, sex and race as predictors. Also, saving the output of `glm` as an R object.
```{r, warning = FALSE, message=FALSE}
baltimore_logistic = homicide_df %>%
    filter(city_state == "Baltimore, MD") %>%
    glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
```

#### Applying the `broom::tidy` to the above object; Obtaining the estimate and confidence interval of the adjusted odds ratio, for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r, warning = FALSE, message=FALSE}
baltimore_logistic %>% 
  broom::tidy(conf.int = T) %>% 
  mutate(OR = exp(estimate),
         CI_Lower = exp(conf.low),
         CI_Upper = exp(conf.high)) %>%
  select(term, log_OR = estimate, OR,CI_Lower,CI_Upper, p.value) %>% 
  knitr::kable(align = "lrr",
               col.names = c("Term","Estimate","OR", "95% CI Lower", "95% CI Upper", "P-value"),
               digits = 3)
```

* Keeping all other variables fixed, in Baltimore, MD, homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female.

## Running `glm` for each of the cities in the dataset; Extracting the adjusted odds ratio and CI, for solving homicides comparing male victims to female victims; 
```{r, warning = FALSE, message=FALSE}
allcities_logistic = homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())),
    results = map(models, ~broom::tidy(.x, conf.int = T))) %>% 
  select(-data, -models) %>% 
  unnest(results)
```

```{r, warning = FALSE, message=FALSE}
allcities_or = allcities_logistic %>%
    mutate(term = fct_inorder(term),
         OR = exp(estimate),
         ci_lower = exp(conf.low),
         ci_upper = exp(conf.high)
         ) %>%
    select(city_state, term, log_OR = estimate, OR, ci_lower, ci_upper,p.value) %>%
    filter(term == "victim_sexMale")
    
    allcities_or %>% knitr::kable(align = "lrr",
               col.names = c("City, State","Term","Estimate","OR", "95% CI Lower", "95% CI Upper", "P-value"),
               digits = 3)
```

#### Creating plot that shows the estimated ORs and CIs for each city
```{r, warning = FALSE, message=FALSE}
allcities_or %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
   labs(
    title = "Estimated ORs and CIs for solved homicides of each city compared with female victim",
    x = "City, State",
    y = "Estimated ORs"
  )
```

* Holding all other variables constant, homicide cases with male victims are generally less likely to be solved compared to homicide cases with female victims because for most cities, the OR and its CI is less than 1 compared to female victims. However, there are some cities where the CI includes 1, meaning that homicide cases with male and female victims are solved at no apparent difference.

## Problem 3

#### Loading and cleaning the data
```{r, warning = FALSE, message=FALSE}
birthweight_raw = read_csv("./data/birthweight.csv")

birthweight_df = birthweight_raw %>% 
    janitor::clean_names() %>%
    mutate(
        across(.cols = c(babysex, frace, malform, mrace), as.factor)
        ) %>%
  mutate(
      babysex = ifelse(babysex == "1", "male","female"),
      malform = ifelse(malform == "0", "absent","present"),
      frace = recode(frace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
      mrace = recode(mrace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other")
         )
```

* `babysex`,`frace`,`malform` and `mrace` were converted from numeric to factor, because they are categorical variables.

* These categorical data were applied `recode` to make it more intuitive.

#### Missing data check
```{r, warning = FALSE, message=FALSE}
skimr::skim(birthweight_df)[c(2,4)] %>% t() %>% knitr::kable()
```

* There is no missing data. The cleaned dataset contains `r nrow(birthweight_df)` observations and `r ncol(birthweight_df)` variables: `r names(birthweight_df)`

#### The Correlations with Birth Weight
```{r, warning = FALSE, message=FALSE}
birthweight_raw %>% 
  cor_mat() %>%
  cor_gather() %>%
  filter(var1 %in% "bwt") %>%
  filter(!var2 %in% "bwt") %>%
  mutate(
    sig_p = ifelse(p < 0.01, T, F),
    cor_if_sig = ifelse(p < 0.01, cor, NA)
    ) %>% 
  ggplot(aes(
    x = var1, 
    y = var2, 
    fill = cor,
    label = round(cor_if_sig, 2))) + 
  geom_tile(color = "white") +   
  geom_text(
    color = "white",
    size = 4
  ) + 
  scale_x_discrete(
    labels = c("Birth Weight")
  ) + 
  labs(
    x = "Outcome",
    y = "Predictors",
    title = "Correlations between predictors and outcome",
    subtitle = "significant predictors at significance level 0.01",
    fill = "Correlation"
  )
```

* Based on t-test, the variables with p-value less than 0.01 are to be selected as potential predictors. To be simple, the continuous are to be selected.

#### Based on the correlation plot, and intuitively, `babysex`, `bhead`, `gaweeks`, `blength`, `wtgain`, `ppwt` are to be included as potential predictors; Hence we can build models on every subsets of these variables and choose the optimal one with the lowest BIC.
```{r, warning = FALSE, message=FALSE}
# find all possible subsets of variables
subsets = unlist(lapply(1:6, combn, x = c("babysex", "bhead", "blength", "wtgain", "ppwt","gaweeks"),  simplify = F), recursive = F)

calc_BIC = function(variables){
  
    formula = as.formula(paste("bwt", paste(variables, collapse = " + "), sep = "~"))
  
    model = lm(formula, birthweight_df)
  return(broom::glance(model) %>% pull("BIC"))
}

# calculate BIC value of every model
BICs = map(subsets, calc_BIC) %>% as_vector()

# choose model with the lowest BIC
index = which(BICs == min(BICs))[[1]]
variables = subsets[[index]]
formula = as.formula(paste("bwt", paste(variables, collapse = " + "), sep = "~"))
fit_optimal = lm(formula, birthweight_df)

birthweight_df %>% 
  add_residuals(fit_optimal) %>% 
  add_predictions(fit_optimal) %>% 
  ggplot(aes(x = pred, y =resid)) +
  geom_point() +
  labs(
    x = "Fitted values",
    y = "Residuals",
    title = "Model residuals vs fitted values"
  )
```

#### Collinearity Check for continuous variables
```{r, warning = FALSE, message=FALSE}
selected_variables =
  birthweight_df %>%
  select(bhead, blength, gaweeks, wtgain, ppwt, bwt)

chart.Correlation(selected_variables, method = "pearson")
```

* From the plot, among predictors, `bhead` vs `blength` or `gaweeks`, `blength` vs `gaweeks` showed a potential collinearity.

#### Final selection of continuous predictors, and intuitively, we can also include `babysex`.
```{r, warning = FALSE, message=FALSE}
selected_variables =
  birthweight_df %>%
  select(bhead, blength, gaweeks, bwt, babysex, mrace, wtgain, ppwt)
```

#### Fitting the linear model with finally selected predictors
```{r, warning = FALSE, message=FALSE}
fit_final <- lm(bwt ~ bhead + blength + gaweeks + babysex + mrace + wtgain + ppwt+ bhead:blength +
             bhead:blength:gaweeks, 
           data = selected_variables)
summary(fit_final) %>% 
  broom::tidy() %>%
  select(term, estimate, p.value)
summary(fit_final) %>% 
  broom::glance()
```

#### Residual plot
```{r, warning = FALSE, message=FALSE}
selected_variables %>%
  add_residuals(fit_final) %>%
  add_predictions(fit_final) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Plot of the model residuals against fitted values",
       x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

#### Comparison in terms of the cross-validated prediction error
```{r, warning = FALSE, message=FALSE}
set.seed(2022)
cv_dataset = selected_variables %>%
    crossv_mc(n = 100,test = 0.2)
  
cv_df = cv_dataset %>%
    mutate(
        train = map(train, as_tibble),
        test = map(test, as_tibble))

cv_df = cv_df %>%
    mutate(
        linear_mod1  = map(train, ~lm(bwt ~ bhead + blength + gaweeks + babysex + mrace+ wtgain + ppwt + bhead:blength + bhead:blength:gaweeks, data = .x)),
        linear_mod2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
        linear_mod3  = map(train, ~lm(bwt ~ bhead + blength + babysex, data = .x))) %>%
    mutate(
        rmse_my_model = map2_dbl(linear_mod1, test, ~rmse(model = .x, data = .y)),
        rmse_given_model1 = map2_dbl(linear_mod2, test, ~rmse(model = .x, data = .y)),
        rmse_given_model2 = map2_dbl(linear_mod3, test, ~rmse(model = .x, data = .y)))
```

#### Root Mean Square Error in the three models
```{r, warning = FALSE, message=FALSE}
cv_df %>% 
    select(starts_with("rmse")) %>% 
    pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
    mutate(model = fct_inorder(model)) %>% 
    ggplot(aes(x = model, y = rmse)) + 
    geom_boxplot() +
    labs(title = "Prediction Error Distributions across Models", x = "Models", y = "Root Mean Square Error")+
    scale_x_discrete(
        labels = c("My Model", "Given Model 1", "Given Model 2")) +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

* Based on cross-validation, my model has the relatively lowest root mean square error and therefore my model works better. While the first given model with predictors **length at birth** and **gestational age** has the highest cross-validated RMSE.
